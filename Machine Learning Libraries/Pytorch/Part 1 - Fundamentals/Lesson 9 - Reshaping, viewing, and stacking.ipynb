{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reshaping, stacking, and viewing tensors\n",
    "\n",
    "* Reshaping - reshapes a tensor to a desired shape (help with tensor shape problems)\n",
    "    * View - Returns a view of a input tensor of certain shape, but keeps the same memory/(shows the same tensor but from a different shape)\n",
    "* Stacking - Combines multiple stacks on top of each other (vstack) or side by side(hstack) (https://pytorch.org/docs/stable/generated/torch.stack.html)\n",
    "\n",
    "(All work in some way to manipulate the tensor in some way that ends with the tensor shape(or dimension) being changed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]), torch.Size([9]))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 1. creating a tensor\n",
    "x = torch.arange(1., 10.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Reshaping\n",
    "- Adding an extra dimension\n",
    "(when reshaping you have to make sure that the new dimensions match the tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 7]' is invalid for input of size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[41], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xReshape \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m xReshape, xReshape\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# here the dimensions of 1,7 are invalid for size 9 as there aren't enough \u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# we are trying to squeeze 9 elements into 7 spaces, which doesn't divide equally, so the new shape has to be an factor of the size\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[1, 7]' is invalid for input of size 9"
     ]
    }
   ],
   "source": [
    "xReshape = x.reshape(1,7)\n",
    "\n",
    "xReshape, xReshape.shape\n",
    "\n",
    "# here the dimensions of 1,7 are invalid for size 9 as there aren't enough \n",
    "# we are trying to squeeze 9 elements into 7 spaces, which doesn't divide equally, so the new shape has to be an factor of the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3.],\n",
       "         [4., 5., 6.],\n",
       "         [7., 8., 9.]]),\n",
       " torch.Size([3, 3]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# so instead of the dimensions (1,7), instead the dimensions of (1,9) works as 1 * 9 fits into the total number of elements in the tensor from 1-9\n",
    "# so any new dimensions that are an multiple of \n",
    "\n",
    "xReshape = x.reshape(3,3)\n",
    "\n",
    "xReshape, xReshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1.],\n",
       "         [2.],\n",
       "         [3.],\n",
       "         [4.],\n",
       "         [5.],\n",
       "         [6.],\n",
       "         [7.],\n",
       "         [8.],\n",
       "         [9.]]),\n",
       " torch.Size([9, 1]))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xReshape = x.reshape(9, 1)\n",
    "\n",
    "xReshape, xReshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[2, 9]' is invalid for input of size 9",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m xReshape \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m9\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# 2 by 9 is invalid as 2 times 9 is greater than than the total elements of the tensor (is double the amonut of elements) is greater than the shape of tensor x\u001b[39;00m\n\u001b[0;32m      5\u001b[0m xReshape, xReshape\u001b[38;5;241m.\u001b[39mshape\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[2, 9]' is invalid for input of size 9"
     ]
    }
   ],
   "source": [
    "xReshape = x.reshape(2,9)\n",
    "\n",
    "# 2 by 9 is invalid as 2 times 9 is greater than than the total elements of the tensor (is double the amonut of elements) is greater than the shape of tensor x\n",
    "\n",
    "xReshape, xReshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Changing the view\n",
    "- Is done by using the pytorch function .view(x, y)\n",
    "    - with the first parameter x being tied to the number of rows\n",
    "    - and with the second parameter being tied to the number of cols\n",
    "- Is similar to reshape: but the use case would be to only visualize the newly changed tensor and not change "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.]]), torch.Size([1, 9]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = x.view(1, 9)\n",
    "# is similar to reshaping the data --> follows the same \n",
    "\n",
    "z, z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 5., 5., 5., 5., 5., 5., 5., 5.]]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7., 8., 9.]))"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#changing z will also change x as the tensor view shares the same memory as the orginal\n",
    "z[0][0] = 5\n",
    "z, x\n",
    "\n",
    "# as shown in the output, when the first element in the z tensor is changed, the first tensor in the x tensor will be also changed to the same value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Stacking Tensors\n",
    "- on top of each other: takes a list of tensors, and then concatenates a sequence of said tensors along a new dimension\n",
    "- all tensors have to be of the same size (all the tensors have to be listed in a [])\n",
    "- the default is dimension 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xStack = torch.stack([x, x, x, x], dim = 0)\n",
    "# creating a stack with 5 x's, and setting the dimension to the default of 0, and stacks vertically\n",
    "\n",
    "xStack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.],\n",
       "        [1., 2., 3., 4., 5., 6., 7., 8., 9.]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xStack = torch.stack([x, x, x, x], dim = 0)\n",
    "# creating a stack with 5 x's, and setting the dimension to 1, and stacks vertically\n",
    "\n",
    "xStack\n",
    "# first 0th index is the 0th tensor"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
