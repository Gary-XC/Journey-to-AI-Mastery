{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up Loss Function And Optimizers\n",
    "\n",
    "Resources: https://pytorch.org/docs/stable/nn.html#loss-functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data points imported from lesson 19\n",
    "\n",
    "weight = 0.91\n",
    "bias = 0.3\n",
    "\n",
    "# Create\n",
    "start = 0\n",
    "end = 1\n",
    "step = 0.02\n",
    "# parameters for creating the tensor: X\n",
    "\n",
    "X = torch.arange(start, end, step).unsqueeze(dim = 1)\n",
    "# adds an extra dimension so theres an extra square bracket and each element of X, y are on different lines in the output for betting viewing\n",
    "y = weight * X + bias # is Linear Regression Formula\n",
    "\n",
    "\n",
    "trainSplit = int(0.8 * len(X)) # creating the train split by multiplying the upper bounds of the train split by the length of X to get the total number\n",
    "\n",
    "XTrain, yTrain = X[:trainSplit], y[:trainSplit] # indexing to get all samples up until the trainsplit\n",
    "XTest, yTest = X[trainSplit:], y[trainSplit:] # indexing to get all the samples from the trainsplit onwards, or what is left over after the trainsplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code imported from lesson 19\n",
    "# using matplotlib to visualize the data points\n",
    "\n",
    "def plotPredictions(trainData = XTrain, \n",
    "                    trainLabels = yTrain, \n",
    "                    testData = XTest, \n",
    "                    testLabels = yTest, \n",
    "                    prediction = None):\n",
    "# Plots training data, test data and comparing predictions\n",
    "\n",
    "    plt.figure(figsize = (10, 7)) \n",
    "    \n",
    "    plt.scatter(trainData, trainLabels, c = \"b\", label = \"Training Data\")\n",
    "\n",
    "    plt.scatter(testData, testLabels, c = \"g\", label = \"Testing Data\")\n",
    "\n",
    "    if prediction is not None:\n",
    "        plt.scatter(testData, prediction, c = \"r\", label = \"Prediction\")\n",
    "\n",
    "    plt.legend(prop = {\"size\": 14})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model imported from lesson 20\n",
    "\n",
    "class LinearRegressionModel(nn.Module): # almost everything in Pytorch inherints from nn.Module, and can be considered the building blocks for pytorch\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.weights = nn.Parameter(torch.randn(1,\n",
    "                                                requires_grad=True, # grad = True is set by default, one of the main algorithms for predictions\n",
    "                                                dtype= torch.float))\n",
    "        \n",
    "        self.bias = nn.Parameter(torch.randn(1,\n",
    "                                             requires_grad= True,\n",
    "                                             dtype= torch.float))\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor: # -> means the return value, very similar to java, but for python its included outside of the method\n",
    "        # 'x' is the input data\n",
    "        return self.weights * x + self.bias # linear regression formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code imported from lesson 22\n",
    "# creating a random seed\n",
    "torch.manual_seed(246)\n",
    "\n",
    "# creating an instance of the model which is a subclass of nn.Module\n",
    "model0 = LinearRegressionModel() # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('weights', tensor([-0.7062])), ('bias', tensor([1.4592]))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# checking for the models parameters, models that the models sets itself\n",
    "\n",
    "model0.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "L1Loss()"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# setting up a loss function\n",
    "\n",
    "lossFnc = nn.L1Loss()\n",
    "lossFnc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    differentiable: False\n",
       "    foreach: None\n",
       "    lr: 0.01\n",
       "    maximize: False\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Setting up an optimizer (using SDG or stochastic gradient descent)\n",
    "\n",
    "optimizer = torch.optim.SGD(params=model0.parameters(),\n",
    "                            lr= 0.01, # learning rate - is the most important hyperparameter that the programmer can set\n",
    "                            )\n",
    "optimizer\n",
    "\n",
    "# starts randomly adjusting the values, and once its found random steps that minimize the loss value, then it will continue adjusting the values in that direction\n",
    "# say if after increasing the weight, it reduces the loss, then the model will continue to increase the weights until the weights no longer reduce the losses anymore"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
